<!DOCTYPE html>
    <html lang="en">
      <head>
<meta charset="utf-8" />

<title>Inferring Concept Drift Without Labeled Data</title>
<meta name="description" content="An online research report on concept drift by Cloudera Fast Forward Labs." />

<meta property="og:title" content="Inferring Concept Drift Without Labeled Data" />
<meta property="og:description" content="An online research report on concept drift by Cloudera Fast Forward Labs." />
<meta property="og:image" content="https://concept-drift.fastforwardlabs.com/figures/ff22-cover-splash.png" />
<meta property="og:url" content="https://concept-drift.fastforwardlabs.com" />
<meta name="twitter:card" content="summary_large_image" />

<meta name="viewport" content="width=device-width" />
<link rel="icon" type="image/x-icon" href="favicon.ico" />

<style type="text/css">
    
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Bold.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Bold.woff') format('woff');
    font-weight: bold;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-BoldItalic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-BoldItalic.woff') format('woff');
    font-weight: bold;
    font-style: italic;
  }
  
    * {
      box-sizing: border-box;
    }
    html {
      background: #fff;
      font-family: "Plex Sans", serif, sans-serif;
      font-size: 17.5px;
      line-height: 28px;
    }
    body {
      margin: 0;
    }
    .content {
      max-width: 64ch;
      padding-left: 2ch;
      padding-right: 2ch;
      margin: 0 auto;
      display: block;
      padding-bottom: 0px;
    }
   p, ul, ol {
      margin: 0;
    }
    ul, ol {
      padding-left: 3ch;
    }
  p {
   // text-indent: 3ch;
}
    li p:first-child {
      text-indent: 0;
    }

    #pdf-logo {
      display: none;
    }

   hr {
      margin: 0;
      border-top-color: black;
      margin-top: -0.5px;
      margin-bottom: 27.5px;
    }
  
h1, h2, h3, h4, h5, h6, button { font-size: inherit; line-height: inherit; font-style: inherit; font-weight: inherit; margin: 0; font-feature-settings: "tnum"; border: none; background: transparent; padding: 0;  }
button:focus, button:hover {
  background: rgba(0,0,0,0.125);
  outline: none;
}
h1 {
  font-size: 42px;
  line-height: 56px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h2 {
  font-size: 31.5px;
  line-height: 42px;
  font-weight: bold;
  margin-top: 28px;
  margin-bottom: 14px;
}
h3 {
  font-size: 26.25px;
  line-height: 35px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h4 {
  font-size: 21px;
  line-height: 28px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h5 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-weight: bold;
}
h6 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-style: italic;
}
p {
  margin-bottom: 14px;
}
.content {
  position: relative;
  }
figure {
  margin: 0;
  margin-top: 14px;
  margin-bottom: 28px;
  display: block;
  position: relative;
  page-break-inside: avoid;
}
blockquote {
  margin: 0;
   margin-top: 14px;
  margin-bottom: 14px;
margin-left: 2ch;
}
blockquote + blockquote {
  margin-top: 0;
}
figcaption {
  font-family: "Plex Mono", serif, monospace;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 21px;
}
.info {
  background: #efefef;
  padding-left: 2ch;
  padding-right: 2ch;
  padding-top: 14px;
  padding-bottom: 14px;
  margin-bottom: 28px;
}
.info p:last-child {
  margin-bottom: 0;
}
img {
  display: block;
  position: relative;
  max-width: 100%;
  margin: 0 auto;
  page-break-inside: avoid;
}
code {
  font-size: 0.9em;
  line-height: 1.2;
  background: rgba(0,0,0,0.125);
  padding: 0 0.3em;
}
pre {
  font-size: 0.9em;
  line-height: 1.2;
  background: rgba(0,0,0,0.125);
  overflow-x: scroll;
  max-width: 100%;
  padding-left: 1ch;
  padding-right: 1ch;
  padding-top:0.625em;
  padding-bottom:0.625em;
}
pre code {
  background: transparent;
}

table {
  min-width: 100%;
  text-align: left;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 18.900000000000002px;
  border-collapse: collapse;
}
table, th, td {
  border: solid 1px black;
}
td {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  valign: top;
  vertical-align: top;
}
th {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  vertical-align: top;
  background: #efefef;
}
table ul, table ol {
  list-style-position: inside;
  padding-left: 0;
}

  a {
    color: inherit;
  }
  .table-of-contents {
    background: #efefef;
    position: fixed;
    left: 0;
    top: 0;
    width: 32ch;
    height: 100vh;
    overflow-y: auto;
    background: #efefef;
      // background: rgba(230,230,230,0.85);
      //   backdrop-filter: blur(5px);
  }
  body {
    padding-left: 32ch;
  }
  p:empty {
    display: none;
  }
  ul, ol {
  margin-bottom: 14px;
  }

  #report-iso {
    display: none;
  }

.table-of-contents {
    counter-reset: chapters;
}
 .table-of-contents ul {
    list-style: none;
    padding-left: 0;
    margin-bottom: 0;
  }
 .table-of-contents > ul {
  }
 .table-of-contents > ul > li {
    font-weight: bold;
  }
 .table-of-contents > ul > li {
    font-weight: bold;
   margin-bottom: 14px;
  }

 .table-of-contents > ul > li > ul > li {
    font-weight: normal;
    font-style: normal;
    text-transform: none;
    letter-spacing: 0;
    margin-left: 0;
  }
 .table-of-contents > ul > li > ul > li > ul > li {
    font-weight: normal;
    font-style: italic;
  }
 .table-of-contents a {
    text-decoration: none;
  }
  .table-of-contents a:hover {
    text-decoration: underline;
  }
 sup {
  }
  .table-of-contents ul a {
    display: block;
    padding-left: 24px;
    text-indent: -8px;
    padding-right: 16px;
  }
  .table-of-contents ul li a.active {
    position: relative;
    background: #ddd;
    // text-decoration: line-through;
  }

 .table-of-contents > ul > li > ul > li > a {
    font-size: 15.75px;
      line-height: 25.2px;
    // padding-left: 4ch;
  }
  .table-of-contents > ul > li > ul > li > ul > li > a {
    padding-left: 5ch;
  }

h1 {
    counter-reset: chp;
}
h2 {
  position: relative;
  display: block;
  page-break-before: always;
  padding-top: 28px;
}
  .toc-desktop-hidden .table-of-contents {
    width: auto;
  }
  .toc-desktop-hidden #contents-label {
    display: none;
  }
  .toc-desktop-hidden .table-of-contents ul {
    display: none;
  }
  body.toc-desktop-hidden {
    padding-left: 5ch;
  }
  body:before {
    content: " ";
    height: 28px;
    width: 96ch;
    background: black;
    position: absolute;
    left: 0;
    top: 0;
    z-index: 999;
    display: none;
  }
    #toc-header {
      margin-top: 14px;
      margin-bottom: 14px;
      margin-left: 1ch;
      margin-right: 1ch;
    }

  @media screen and (max-width: 1028px) {
    h1 {
      font-size: 36.75px;
      line-height: 49px;
      font-weight: bold;
      margin-top: 14px;
      margin-bottom: 14px;
    }
    .table-of-contents ul li {
    }

    #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
    }

    body {
      padding-left: 0;
      padding-top: 42px;
    }
    .content {
        overflow-wrap: break-word;
        word-wrap: break-word;
    }
    #contents-label {
      display: none;
    }
    .table-of-contents {
      height: auto;
      width: 100%;
      z-index: 3;
    }
  body.toc-mobile-show .content:before {
      content: "";
      position: fixed;
      left: 0;
      top: 0;
      bottom: 0;
      right: 0;
      background: rgba(0,0,0,0.25);
      z-index: 2;
      border-top: solid 42px #aaa;
    }

    .table-of-contents > ul {
      display: none;
    }
   body.toc-mobile-show {
      overflow: hidden;
    }
    body.toc-mobile-show #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
      position: relative;
    }
    body.toc-mobile-show .table-of-contents {
      width: 32ch;
      height: 100vh;
      max-width: calc(100% - 4ch);
      overflow: auto;
    }
   body.toc-mobile-show .table-of-contents > ul {
      display: block;
      padding-bottom: 28px;
      position: relative;
    }
    body.toc-mobile-show #contents-label {
      display: inline;
      position: relative;
    }
  }
}
</style>
<script>
    function inViewport(elem) {
      let bounding = elem.getBoundingClientRect();
      return (
        bounding.top >= 0 &&
        bounding.left >= 0 &&
        bounding.bottom <= (window.innerHeight || document.documentElement.clientHeight) &&
        bounding.right <= (window.innerWidth || document.documentElement.clientWidth)
      );
    };

    function setActive(target_id) {
      let selector = '.table-of-contents ul li a[href="#' + target_id + '"]'
      let link = document.querySelector(selector)
      if (link !== null) {
        link.className = 'active'
      }
    }

    window.addEventListener("load", (event) => {
      let headings = document.querySelectorAll('h2, h3');
      let links = document.querySelectorAll('.table-of-contents ul li a')

      observer = new IntersectionObserver((entry, observer) => {
        if (entry[0].intersectionRatio === 1) {
          for (let link of links) {
            link.className = ''
          }
          let target_id = entry[0].target.getAttribute('id')
          setActive(target_id)
        }
      }, { threshold: 1, rootMargin: "0px 0px -50% 0px" });

      let first = true
      for (let heading of headings) {
        if (first && inViewport(heading)) {
          setActive(heading.getAttribute('id'))
          first = false
        }
        observer.observe(heading);
      }

      document.querySelector('#toggle_contents').addEventListener('click', () => {
        let body = document.body
        if (window.innerWidth > 1027) {
          let hidden_class = "toc-desktop-hidden"
          if (body.className === hidden_class) {
            body.className = ''
          } else {
            body.className = hidden_class
          }
        } else {
          let show_class = "toc-mobile-show"
          if (body.className === show_class) {
            body.className = ''
          } else {
            body.className = show_class
          }
        }
      })

      for (let link of links) {
        link.addEventListener('click', (e) => {
          let href = e.target.getAttribute('href')
          let elem = document.getElementById(href.slice(1))
          window.scroll({
            top: elem.offsetTop - 28,
            left: 0,
            behavior: 'smooth'
          })
          if (window.innerWidth < 1028) {
            document.body.className = ''
          }
          e.preventDefault()
        })
      }

      document.querySelector('.content').addEventListener('click', () => {
        if (window.innerWidth < 1028) {
          document.body.className = ''
        }
      })
      document.querySelector('.table-of-contents').addEventListener('click', (e) => {
        e.stopPropagation()
      })

      let mediaQueryList = window.matchMedia("(max-width: 1028px)");
      function handleBreakpoint(mql) {
        // clear any left over toggle classes
        document.body.className = ''
      }
      mediaQueryList.addListener(handleBreakpoint);
    }, false);
  </script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-157475426-14', 'auto');
  ga('send', 'pageview');

  window.addEventListener('load', function() {
    document.getElementById('report-pdf-download').addEventListener('click', function() {
      ga('send', {
        hitType: 'pageview',
        page: '//FF22-Concept_Drift-Cloudera_Fast_Forward.pdf'
      });
    });
  })

</script>
<!-- End Google Analytics -->
</head>
      <body>
        <div class="content" style="position: relative;">
          <div id="html-logo" style="margin-top: 28px; line-height: 0; display: flex;">
            <a href="https://www.cloudera.com/products/fast-forward-labs-research.html"><img alt="Cloudera Fast Forward Labs" style="display: block; height: 14px; margin-bottom: 7px;" src='/figures/cloudera-fast-forward.png' /></a>
          </div>
          <div id="pdf-logo" style="margin-top: 28px; ">
            <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Cloudera Fast Forward Labs</a>
          </div>
          <h1 id="inferring-concept-drift-without-labeled-data">Inferring Concept Drift Without Labeled Data</h1>
<p>FF22 · <em>Aug 2021</em></p>
<figure><img src="figures/ff22_cover_splash.png" alt=""></figure>
<p><em>This is an applied research report by <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Cloudera Fast Forward</a>. We write reports about emerging technologies,
and conduct experiments to explore what’s possible. Read our full report about <em>Inferring Concept Drift Without Labeled Data</em> below, or <a href="/FF22-Concept_Drift-Cloudera_Fast_Forward.pdf" target="_blank" id="report-pdf-download">download the PDF</a>. You can view and download the code accompanying our concept drift experiments <a href="https://github.com/fastforwardlabs/concept-drift">on Github</a>.</em></p>
<p><div class="table-of-contents"><div id="toc-header" style="display: flex; font-weight: bold; text-transform: uppercase;">
     <div><button id="toggle_contents" style="padding-left: 0.5ch; padding-right: 0.5ch; cursor: pointer; position: relative; top: -1px;">☰</button><span id="contents-label" style="margin-left: 0;"> Contents</span></div>
  </div><ul><li><a href="#introduction">Introduction</a></li><li><a href="#background">Background</a><ul><li><a href="#what-is-concept-drift%3F">What is concept drift?</a></li><li><a href="#use-cases">Use Cases</a></li></ul></li><li><a href="#modeling-session-based-recommenders">Modeling Session-based Recommenders</a><ul><li><a href="#treating-it-as-a-nlp-problem">Treating it as a NLP problem</a></li><li><a href="#how-can-we-use-this-for-nep%3F">How can we use this for NEP?</a></li></ul></li><li><a href="#evaluating-session-based-recommenders">Evaluating Session-based Recommenders</a><ul><li><a href="#evaluation-metrics">Evaluation metrics</a></li></ul></li><li><a href="#experiments">Experiments</a><ul><li><a href="#data">Data</a></li><li><a href="#setup">Setup</a></li><li><a href="#hyperparameters-matter">Hyperparameters Matter</a></li><li><a href="#hyperparameter-tuning-with-ray">Hyperparameter Tuning with Ray</a></li><li><a href="#challenges">Challenges</a></li></ul></li><li><a href="#overall-considerations">Overall Considerations</a><ul><li><a href="#session-related-issues">Session-related issues</a></li><li><a href="#what-determines-a-good-recommendation%3F">What determines a good recommendation?</a></li><li><a href="#online-evaluation">Online evaluation</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul></div></p>
<h2 id="introduction">Introduction</h2>
<p>After iterations of development and testing, deploying a well-fit machine learning model often feels like the final hurdle for an eager data science team. In practice however, a trained model is never final, and this milestone marks just the beginning of the perpetual maintenance race that is production machine learning. This is because most machine learning models are static, but the world we live in is dynamic. More specifically, the ability of a trained model to generalize relies on an important assumption of stationarity - meaning the data upon which a model is trained and tested are independent and identically distributed (i.i.d). In real-world environments, this assumption is often violated as human behavior and consequently the systems we aim to model are dynamically changing all time.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<figure><img src="" alt="Figure 1: Examples of machine learning tasks where the effects of concept drift are prominent"><figcaption>Figure 1: Examples of machine learning tasks where the effects of concept drift are prominent</figcaption></figure>
<p>Take for instance the impact of the recent COVID-19 pandemic on algorithm driven businesses like inventory management. Instacart’s model for forecasting in-store product availability <a href="https://fortune.com/2020/06/09/instacart-coronavirus-artificial-intelligence/">dropped from 93% to 61% accuracy</a> due to the drastic change in shopping behavior as consumers stockpiled what previously were infrequently purchased goods. The model was forced to adapt to this transitory shift in it’s prior understanding of the world.</p>
<p>Not all changes are this sudden though. Consider the task of maintaining an email spam filtering service. The core technology consists of a text classification model that picks up on keywords in email content to block spammers. Over time, users will begin to manually report more messages as spam that are not caught by the filter. In this adversarial environment, spammers are continuously adjusting terminology to outwit the deployed spam filters so models must relearn what language constitutes the evolving concept of spam to remain effective.</p>
<p>Or we can look at the job of forecasting energy consumption where historical demand is just one piece of the puzzle. In practice, future demand is driven by a slew of non-stationary forces like climate fluctuations, population growth, or disruptive clean energy tech that necessitate both gradual and sudden model adaptation.</p>
<p>Changes in environmental conditions like these are referred to as concept drift and will cause the predictive performance of a model to degrade over time, making it obsolete for the task it was initially intended to solve.</p>
<figure><img src="" alt="Figure 2: Production model performance will decay over time without adapting to drifting concepts"><figcaption>Figure 2: Production model performance will decay over time without adapting to drifting concepts</figcaption></figure>
<p>To combat this divergence between static models and dynamic environments, teams often adopt an adaptive learning strategy that is triggered by the detection of a drifting concept. Supervised drift detection is generally achieved by monitoring a performance metric of interest (such as accuracy) and alerting a retraining pipeline when the metric falls below some designated threshold.</p>
<p>While this strategy proves to be effective, there are several limitations that often prevent its use in practice. Namely, it requires immediate access to an abundance of labels at inference time to quantify a change in system performance - a requirement that may be cost prohibitive or outright impossible in many real-world machine learning applications.</p>
<p>In this report, we explore broadly applicable approaches for dealing with concept drift when labeled data is not readily accessible. We’ll start by defining what we mean by concept drift and frame the limitations of supervised methods for detecting it. Then, we’ll discuss why true unsupervised concept drift detection is not possible, and explore several alternative methods for dealing with it. Finally, we’ll share our experimental results supporting the proposed methods, and wrap up with a discussion of considerations and limitations.</p>
<h2 id="background">Background</h2>
<h3 id="what-is-concept-drift%3F">What is concept drift?</h3>
<p>Most machine learning systems today operate in a batch paradigm where they probe a historical data set to develop a model that reflects the world as it was at the time of training. But as we’ve seen, the world is always changing and the complex relationships that a model abstracts are also likely to change over time causing model performance to deteriorate if not accounted for. This phenomenon in which the statistical properties of a target domain change over time is considered concept drift.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>Formally, concept drift between time <code>$t$</code> and $$t+1$$ can be defined as</p>
<p>Let’s say we own a popular online shopping website for workout accessories. Rhonda, a new customer, has been browsing tops, shoes, and weights. Her browsing history looks like this:</p>
<figure><img src="figures/FF19_Artboard_4rev.png" alt="Figure 4:  Rhonda’s browsing history"><figcaption>Figure 4:  Rhonda’s browsing history</figcaption></figure>
<p>What should we recommend to her next? Good recommendations increase the likelihood that Rhonda will see something she likes, click on it, and make a purchase. Poor recommendations will, at best, lead to no new revenue, but-even worse-could give her a negative customer experience. (You know this feeling: when a website keeps recommending something to you that you have already bought, or something that you’ve never really wanted, your impression of that website diminishes!)</p>
<p>We’ll consider Rhonda’s recent browsing history as a “session.” Formally, a session is composed of multiple user interactions that happen together in a continuous period of time—for instance, products purchased in a single transaction. Sessions can occur on the same day, or across several days, weeks, or months.</p>
<p>Our goal is to predict the product within Rhonda’s session that she will like enough to click on. This task is called <strong>next event prediction</strong> (NEP): given a series of events (Rhonda’s browsing history), we want to predict the next event (Rhonda clicking on a product we recommend to her).</p>
<p>In reality, this means that our model might generate a handful of recommendations based on Rhonda’s browsing history; we want to maximize the likelihood that Rhonda clicks on at least one of them. To train a model for this task, we’ll need to use historical browsing sessions from our other existing users to identify trends between products that will help us learn recommendations.</p>
<figure><img src="figures/FF19_Artboard_5.png" alt="Figure 5: Historical browsing sessions of various lengths"><figcaption>Figure 5: Historical browsing sessions of various lengths</figcaption></figure>
<h3 id="use-cases">Use Cases</h3>
<p>This problem is well-aligned with emerging real-world use cases, in which modeling short-term preferences is highly desirable. Consider the following examples in music, rental, and product spaces.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></p>
<h4 id="music-recommendations">Music recommendations</h4>
<p>Recommending additional content that a user might like while they browse through a list of songs can change a user’s experience on a content platform.</p>
<p>The user’s listening queue follows a sequence. For each song the user has listened to in the past, we would want to identify the songs listened to directly before and after it, and use them to teach the machine learning model that those songs somehow belong to the same context. This allows us to find songs that are similar, and provide better recommendations.<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></p>
<figure><img src="figures/FF19_Artboard_6rev.png" alt="Figure 6: Playlist"><figcaption>Figure 6: Playlist</figcaption></figure>
<h4 id="rental-recommendations">Rental recommendations</h4>
<p>Another powerful and useful application of session-based recommendation systems occurs in any type of online marketplace. For instance, imagine a website that contains millions of diverse rental listings, and a guest exploring them in search of a place to rent for a vacation.<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> The machine learning model in such a situation should be able to leverage what the guest views during an ongoing search, and learn from these search sessions the similarities between the listings. The similarities learned by the model could potentially encode listing features-like location, price, amenities, design taste, and architecture.</p>
<figure><img src="figures/FF19_Artboard_7rev.png" alt="Figure 7: Rental listings"><figcaption>Figure 7: Rental listings</figcaption></figure>
<h4 id="product-recommendations">Product recommendations</h4>
<p>Leveraging emails in the forms of promotions and purchase receipts to recommend the next item to be purchased has also proven to be a strong purchase intent signal.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> Again, the idea here is to learn a representation of products from historical sequences of product purchases, under the assumption that products with similar contexts (that is, surrounding purchases) can help recommend more meaningful and diverse suggestions for the next product a user might want to purchase.</p>
<figure><img src="figures/FF19_Artboard_8rev.png" alt="Figure 8: Email purchase receipts"><figcaption>Figure 8: Email purchase receipts</figcaption></figure>
<p>With these examples in mind, let’s dig deeper into what it takes to design and build a session-based recommendation system for product recommendations, in the context of an online retail website.</p>
<h2 id="modeling-session-based-recommenders">Modeling Session-based Recommenders</h2>
<p>There are many baselines for the next event prediction (NEP) task. The simplest and most common are designed to recommend the item that most frequently co-occurs with the last item in the session. Known as “Association Rules,”<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> this heuristic is straightforward, but doesn’t capture the complexity of the user’s session history.</p>
<p>More recently, deep learning approaches have begun to make waves. Variations of graph neural networks<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> and recurrent neural networks<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> have been applied to the problem with promising results, and currently represent the state of the art in NEP for several use cases. However, while these algorithms capture complexity, they can also be difficult to understand, unintuitive in their recommendations, and not <em>always</em> better than comparably simple algorithms (in terms of prediction accuracy).</p>
<p>There is still another option, though, that sits between simple heuristics and deep learning algorithms. It’s a model that can capture semantic complexity with only a single layer: word2vec.</p>
<h3 id="treating-it-as-a-nlp-problem">Treating it as a NLP problem</h3>
<p>Word2vec? Isn’t that the algorithm that made word embeddings commonplace? Yes! Word2vec uses the co-occurrence of words in a sentence to learn embeddings for each word that capture the semantic meaning of that word. At its core, word2vec is a simple, shallow neural network, with a single hidden layer. In its skip-gram version, it takes as input a word, and tries to predict the context of words around it as the output. For instance, consider this sentence:</p>
<p>“The cat <em>jumped</em> over the puddle.”<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></p>
<p>Given the central word “jumped,” the model will be able to predict the surrounding words: “The,” “cat,” “over,” “the,” “puddle.”</p>
<figure><img src="figures/FF19_Artboard_9.png" alt="Figure 9: Word2vec versions: Skip-Gram vs Continuous Bag of Words"><figcaption>Figure 9: Word2vec versions: Skip-Gram vs Continuous Bag of Words</figcaption></figure>
<p>Another approach—Continuous Bag of Words (CBOW)—treats the words “The,” “cat,” “over,” “the,” and “puddle” as the context, and predicts the center word: “jumped.” For the rest of this report, we will restrict ourselves to the skip-gram model. The Ws in the above diagram represent the weight matrices that control the weight of the successive transformations we apply to the input to get the output. Training this shallow network means learning the values of these weight matrices, which gives us the output that is closest to the training data. Once trained, the output layer is usually discarded, and the hidden layer (also known as the embeddings) is used for downstream processes. These embeddings are nothing but vector representations of each word, such that similar words have vector representations that are close together in the embedding space.</p>
<h3 id="how-can-we-use-this-for-nep%3F">How can we use this for NEP?</h3>
<p>Let’s take another look at Rhonda’s browsing history. We can treat each session as a sentence, with each item or product in the session representing a “word.” A website’s collection of user browser histories (including Rhonda’s) will act as the corpus. Word2vec will crunch over the entire corpus, learning relationships between products in the context of user browsing behavior. The result will be a collection of embeddings: one for each product. The idea is that these learned product embeddings will contain more information than a simple heuristic, and training the word2vec algorithm is typically faster and easier than training more complex, data-hungry deep learning algorithms.</p>
<p>In fact, word2vec can be invoked as a reasonable approach any time we’re faced with a problem that is sequential in nature, and where the order of the sequences contains information. In this case, casting sessions as an NLP problem makes sense because we do have sequential data (user browser histories are typically ordered by time) and the order likely does matter (capturing the user’s interests as they navigate various products). While the basic algorithm can conceptually be applied to other domains, only recently has research explored this explicitly for the NEP task.<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup> <sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup></p>
<h2 id="evaluating-session-based-recommenders">Evaluating Session-based Recommenders</h2>
<p>Evaluation of session-based recommendation systems typically comes in two stages: offline and online evaluation. In offline evaluation, the historical user sessions are typically considered as the “gold standard” for the evaluation. The effectiveness of an algorithm is measured by its ability to predict items withheld from the session. There are a variety of withholding strategies:<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup></p>
<blockquote>
<ol>
<li>withholding the last element of each session,</li>
<li>iteratively revealing each interaction in a session, and</li>
<li>in cases where each user has multiple sessions, withholding the entire final session.</li>
</ol>
</blockquote>
<p>For the purposes of this research report, we have employed withholding the last element of the session. That said, while it is a conceptually simple approach, it may not reflect the user journey throughout a session in the best way. Also, using word2vec for the NEP recommendation task means we are evaluating the embeddings generated by the word2vec model to measure the performance. So, if an item hasn’t been part of the training set, one may have to come up with alternative ways of generating the embedding. (Refer to <a href="#cold-start-in-experiments">Cold Start in Experiments</a> for more details.)</p>
<h3 id="evaluation-metrics">Evaluation metrics</h3>
<p>When looking at time-ordered sequences of user interactions with the items, we split each sequence into train, validation, and test sets. For a sequence containing n interactions, we use the first (<em>n</em>-1) items in that sequence as part of the model training set.  We randomly sample (<em>n</em>-1th, <em>n</em>th) pairs from these sequences for the validation and test sets. For prediction, we use the last item in the training sequence (the <em>n</em>-1th item) as the query item, and predict the <em>K</em> closest items to the query item using cosine similarity between the vector representations. We can then evaluate with the following metrics:</p>
<ul>
<li>
<p><strong>Recall at <em>K</em> (Recall@<em>K</em>)</strong> defined as the proportion of cases in which the ground truth item is among the top <em>K</em> recommendations for all test cases (that is, a test example is assigned a score of 1 if the <em>n</em>th item appears in the list, and 0 otherwise).</p>
</li>
<li>
<p><strong>Mean Reciprocal Rank at <em>K</em> (MRR@<em>K</em>)</strong>, takes the average of the reciprocal ranks of the ground truth items within the top <em>K</em> recommendations for all test cases (that is, if the <em>n</em>th item was second in the list of recommendations, its reciprocal rank would be 1/2). This metric measures and favors higher ranks in the ordered list of recommendation results.</p>
</li>
</ul>
<p>Accuracy, however, is not the only relevant factor when it comes to recommendations. Depending on the problem, we may want to measure how diverse the recommendations are, or if our algorithm generally tends to recommend most popular items. These additional quality metrics, known as coverage (or diversity) and popularity bias, could help us better understand the potential side-effects of the recommender model.</p>
<h2 id="experiments">Experiments</h2>
<p>Let’s put this into practice, and see how to specifically use word2vec for next event prediction (NEP) in order to generate product recommendations. In keeping with our e-commerce example above (Rhonda’s online shopping), we’ve chosen an open source e-commerce dataset that lends itself well to this task. The discussion below details our strategy, experiments, and results (the code for which can be found on <a href="https://github.com/fastforwardlabs/session_based_recommenders">our GitHub repo</a>).</p>
<h3 id="data">Data</h3>
<p>We chose an open domain e-commerce dataset<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup> from a UK-based online boutique selling specialty gifts. This dataset was collected between 12/01/2010 and 12/09/2011 and contains purchase histories for 4,372 customers and 3,684 unique products. These purchase histories record transactions for each customer and detail the items that were purchased in each transaction. This is a bit different from a browsing history, as it does not contain the order of items clicked while perusing the website; it only includes the items that were eventually purchased in each transaction. However, the transactions are ordered in time, so we can treat a customer’s full transaction history as a session. Instead of predicting recommendations for what a customer might click on next, we’ll be predicting recommendations for what that customer might actually <em>buy</em> next. Session definitions are flexible, and care must be taken in order to properly interpret the results (more on this in <a href="#overall-considerations">Overall Considerations</a>).</p>
<p>In this case, we define a session as a customer’s full purchase history (all items purchased in each transaction) over the life of the dataset. Below, we show a boxplot of the session lengths (how many items were purchased by each customer). The median customer purchased 44 products over the course of the dataset, while the average customer purchased 96 products.</p>
<figure><img src="figures/session_lengths.png" alt="Figure 10: Session length in the Online Retail Data Set"><figcaption>Figure 10: Session length in the Online Retail Data Set</figcaption></figure>
<p>Another thing to note is the popularity of individual products. Below, we show the log counts of how often each product was purchased. Most products are not very popular and are only purchased a handful of times. On the other hand, a few products are wildly popular and purchased thousands of times.</p>
<figure><img src="figures/product_counts_arrow.png" alt="Figure 11: Log counts of each product in the Online Retail Data Set"><figcaption>Figure 11: Log counts of each product in the Online Retail Data Set</figcaption></figure>
<p>This dataset has already been preprocessed (e.g., personally identifying information has already been removed.) The only additional preprocessing we performed was to remove entries that did not contain a customer ID number (which is how we define a session).</p>
<h3 id="setup">Setup</h3>
<p>In NEP, we consider a user’s history to recommend items for the future—but, when training models for recommendation, all the data is historical. In order to mimic “real life” behavior, we’ll pretend that we only have access to the user’s first <em>n</em>-1 purchased items, and use those to try to predict the <em>n</em>th item purchased.</p>
<p>To visualize this, let’s go back to Rhonda’s historical browsing information, collected while she was using our site. We’ll use the highlighted items as our training set to learn product representations, which will be used to generate recommendations. Recommendations are typically based on the most recent interaction by the user, called the <strong>query item</strong>. In this case, we’ll treat the last item (“cap” in our highlighted set of items below) as the query item, and use that to generate a set of recommendations.</p>
<p>The item outside of the highlighted box (in this case, a “water bottle”) will be the ground truth item, and we’ll then check whether this item is contained within our generated recommendations.</p>
<figure><img src="figures/FF19_Artboard_10rev.png" alt="Figure 12: Rhonda’s session, wherein the first n-1 items highlighted in a green box act as part of the training set, while the item outside is used as ground truth for the recommendations generated."><figcaption>Figure 12: Rhonda’s session, wherein the first <em>n</em>-1 items highlighted in a green box act as part of the training set, while the item outside is used as ground truth for the recommendations generated.</figcaption></figure>
<p>To put it more concretely: for each customer in the Online Retail Data Set, we construct the training set from the first <em>n</em>-1 purchased items. We construct test and validation sets as a series of [query item, ground truth item] pairs. The test and validation sets must be disjoint—that is, each set is composed of pairs with no pairs shared between the two sets (or else we would leak information from our validation into the final test set!).</p>
<p>With this in mind, there is one more preprocessing step that we must apply to our dataset. Namely, we remove sessions that contain fewer than three purchased items. A session with only two, for instance, is just a [query item, ground truth item] pair and does not give us any examples for training.</p>
<p>Once we have our train/test/validation sets constructed, it’s time to train! Training <a href="https://radimrehurek.com/gensim/">Gensim</a>’s word2vec is a one-liner. (For the uninitiated, Gensim is an open-source natural language processing library for training vector embeddings.) We simply pass it the training set and two very important parameters: <code>min_count</code> and <code>sg</code>. <code>min_count</code> is the minimum number of times a word in the vocabulary must be present for word2vec to create an embedding for it. Because we have some rare products, we set this to 1, so that all product IDs in the training set have an embedding. (<code>sg</code> is short for skip-gram, and setting this equal to 1 causes word2vec to use this architecture, as opposed to CBOW.)</p>
<p>Under the hood, word2vec will construct a “vocabulary,” a collection of all unique product IDs, and then learn an embedding for each. Once trained, we can extract the product ID embeddings.</p>
<figure><img src="figures/code_snippet.png" alt=""></figure>
<p>Next, we need to generate recommendations. Given a query item, we’ll generate a handful of recommendations that are the most similar to that item, using cosine similarity. This is the same technique we would use if we wanted to find similar words. Instead of semantic similarity between words, we hope we have learned embeddings that capture the semantic similarity between product IDs that users purchased. Thus, we’ll look for other product IDs that are “most similar” to the query item.</p>
<figure><img src="figures/FF19_Artboard_11.png" alt="Figure 13:  The query item (the last item in a training sequence) is used to generate K product recommendations"><figcaption>Figure 13:  The query item (the last item in a training sequence) is used to generate K product recommendations</figcaption></figure>
<p>Armed with a list of recommendations, we can now score our model by checking whether the corresponding ground truth item is in our list of recommendations.</p>
<figure><img src="figures/FF19_Artboard_12.png" alt="Figure 14: The user’s actual next selection (the final item in the user’s sequence) is considered the ground truth item, and we check whether that item is found in our list of generated recommendations."><figcaption>Figure 14: The user’s actual next selection (the final item in the user’s sequence) is considered the ground truth item, and we check whether that item is found in our list of generated recommendations.</figcaption></figure>
<p>We’ll perform this set of operations for each [query item, ground truth item] pair in our test set, to compute an overall score using Recall@<em>K</em> and MRR@<em>K</em>. The output of the code snippet above resulted in a Recall@10 of 19.7 and MRR@10 of 0.108. These results tell us that nearly 20% of the time, the user’s true selection was included in the list of recommendations we generated.</p>
<h3 id="hyperparameters-matter">Hyperparameters Matter</h3>
<p>In the previous section, we simply trained word2vec using the default hyperparameters-but hyperparameters matter! In addition to the learning rate or the embedding size (hyperparameters likely familiar to many), word2vec has several others which have considerable impact on the resulting embeddings. Let’s see how.</p>
<h4 id="context-window-size">Context window size</h4>
<p>The window size controls the size of the context. Recall our earlier example: “The cat jumped over the puddle.” If the context size were set to 6, the entire sentence would be contained within the <strong>context window</strong>. If we set it to 5, then this sentence would be broken up, and we would instead consider a context like: “The cat jumped over the.” Thus, the context window influences how far apart words can be while still being considered in the same context.</p>
<h4 id="negative-sampling-exponent">Negative sampling exponent</h4>
<p>Word2vec’s training objective is to find word representations that are useful for predicting the surrounding words in the context, by maximizing the average log probability over all possible words.<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> This probability is modeled with the softmax function. Computing the softmax scales with the size of the vocabulary, as such, can become computationally expensive. However, we can approximate the softmax through a technique known as “negative sampling.” In this technique, the model must try to discern between a word that is truly in the context from words that are not part of the context.</p>
<p>Negative samples are chosen from all the other possible words in the corpus based on the frequency distribution of words in that corpus. This frequency distribution is controlled by a hyperparameter called the <strong>negative sampling exponent</strong>. When this value is 1, common words are more likely to be chosen as negative samples (think stopwords like “the”, “it”, “and”). If the value is 0, then each word is equally likely to be chosen as a negative example (this is a uniform distribution where “the” is just as likely as “cordial”). Finally, if the value is -1, then rare words are more likely to be selected as negative examples.</p>
<p>So, we will train the model to positively identify words belonging to the same context by presenting it with pairs, like [“jumped”, “the”], [“jumped”, “cat”], etc. These are positive examples because “the” and “cat” both appear in the same context as “jumped.”  In negative sampling, we will also try to train the model to identify words that are not in the context, by presenting examples like [“jumped”, “airplane”] or [“jumped”, “cordial”]. (These are negative examples because “airplane” and “cordial” are not in the same context as “jumped.”)</p>
<h4 id="number-of-negative-samples">Number of negative samples</h4>
<p>In addition to the negative sampling exponent, another hyperparameter determines how many <strong>negative samples</strong> are provided for each positive example. That is, when showing the model [“jumped”, “cat”] (our positive sample), we could include one, five, or maybe even fifty different negative samples.</p>
<p>The code snippet displayed above uses the default values for each of these hyperparameters. These values were found to produce semantically meaningful representations for words in documents, but we are learning embeddings for products in online sessions. The order of products in online sessions will not have the same structure as words in sentences, so we’ll need to consider adjusting word2vec’s hyperparameters to be more appropriate to the task.</p>
<div style="width: 100%; overflow-x: auto;"><table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Start Value</th>
<th>End Value</th>
<th>Step Size</th>
<th>Configurations</th>
</tr>
</thead>
<tbody>
<tr>
<td>Context window size</td>
<td>1</td>
<td>19</td>
<td>3</td>
<td>7</td>
</tr>
<tr>
<td>Negative sampling exponent</td>
<td>-1</td>
<td>1</td>
<td>0.2</td>
<td>11</td>
</tr>
<tr>
<td>Number of negative samples</td>
<td>1</td>
<td>19</td>
<td>3</td>
<td>7</td>
</tr>
<tr>
<td><strong>Number of Trials</strong></td>
<td></td>
<td></td>
<td></td>
<td>539</td>
</tr>
</tbody>
</table></div>
<figcaption>Table 1: This table shows the main hyperparameters we tuned over. For each one, we show the starting and ending values we tried, along with the step size we used. The total number of trials is computed by multiplying each value in the Configurations column.</figcaption><br/>
<p>Above, we detail the hyperparameters we considered, the values we allowed these hyperparameters to assume, and the total number of trials necessary to test them all in a sweep. If we want to find the best hyperparameters for our dataset, we’ll need to do quite a bit of training—more than 500 different hyperparameter combinations! We <em>could</em> set up our own code, constructing several nested loops to cover all of the possible parameters—or perhaps we could use sklearn’s GridSearch. However, there’s an even better solution.</p>
<h3 id="hyperparameter-tuning-with-ray">Hyperparameter Tuning with Ray</h3>
<p>There are several libraries that make hyperparameter optimization approachable, easy, and <em>scalable</em>. For this cycle, we explored <a href="https://docs.ray.io/en/master/index.html">Ray</a>.</p>
<p>At its core, Ray is a simple, universal API for building distributed applications. Atop this foundation are a handful of libraries designed to address specific machine learning challenges. <a href="https://docs.ray.io/en/master/tune/index.html">Ray Tune</a> provides several desirable features, including distributed hyperparameter sweep, checkpointing, and state-of-the-art hyperparameter search algorithms—all while supporting most major ML frameworks, such as PyTorch, Tensorflow, and Keras.</p>
<p>In our experiments, we tuned over the hyperparameters in the table above for the number of trials specified. Each trial was trained for 100 epochs and was evaluated on the validation set using Recall@10. This resulted in the best hyperparameter configuration for the Online Retail Data Set.</p>
<h4 id="results">Results</h4>
<p>We trained a word2vec model using the best hyperparameters found above, which resulted in a Recall@10 score of 25.18±0.19 on the validation set, and 25.21±.26 for the test set. These scores may not seem immediately impressive, but if we consider that there are more than 3600 different products to recommend, this is far better than random chance!</p>
<h4 id="hyperparameter-optimization-results">Hyperparameter Optimization Results</h4>
<p>For this dataset, there is actually a wide range of values that would work well for this task. In the following figures, we plot the Recall@10 score for each  hyperparameter configuration we tested. Because we tuned over three hyperparameters, we display three figures showing the relationship between pairs of hyperparameters. Essentially, we’ve flattened a 3D space into two dimensions for readability. This means that, for the flattened dimension, we averaged over the Recall@10 scores. For instance, in the left-most figure we plot the <em>number of negative samples</em> against the <em>negative sampling exponent</em>. We average over the Recall@10 scores in the context window dimension, which are the colored points you see in the 2D figure, where yellow indicates a high Recall@10 score and purple is a low score.</p>
<figure><img src="figures/hpsweep_results.png" alt="Figure 15: Results from our hyperparameter sweep: Each panel shows the Recall@10 scores (colored points, where yellow is a high score, and purple is a low score) associated with a unique configuration of hyperparameters. The best hyperparameter values for the Online Retail Data Set are denoted by the light blue circle. Word2vec’s default values are shown by the orange star. In all cases, the orange star is nowhere near the light blue circle, indicating that the default values are not optimal for this dataset."><figcaption>Figure 15: Results from our hyperparameter sweep: Each panel shows the Recall@10 scores (colored points, where yellow is a high score, and purple is a low score) associated with a unique configuration of hyperparameters. The best hyperparameter values for the Online Retail Data Set are denoted by the light blue circle. Word2vec’s default values are shown by the orange star. In all cases, the orange star is nowhere near the light blue circle, indicating that the default values are not optimal for this dataset.</figcaption></figure>
<p>In each figure above, the orange star signifies the default word2vec values, and the light blue circle indicates the best hyperparameter configuration we found during our sweep. In all cases, the best hyperparameters are typically in the upper right quadrant; larger values of these hyperparameters perform better than smaller values for this dataset. And in each case, the default word2vec parameters are near, but outside of, the optimal range of values to maximise performance on Recall@10.</p>
<p>We also find two other items of note. First is the size of the context window. In the middle figure, we see that many values of context window size (y axis) will work pretty well, so long as it’s larger than one. (Note that the bottom row of this figure is quite purple, indicating very low relative scores). When the context window is too small, the model is unable to learn relationships between product IDs, and thus recommendations (and our Recall@10 score) suffer. Context matters!</p>
<p>Second is the number of negative samples. In the figure to the far right, we see that more negative samples lead to much better Recall@10 scores. When we select only a single negative example for each positive example, the model again struggles. It is crucial to provide the model with sufficient negative examples. The model must learn not only what things <em>should</em> be in the same context, but especially what things <em>should not</em> be in the same context.</p>
<h4 id="model-comparisons">Model Comparisons</h4>
<p>Now that we’ve learned the best hyperparameters for our dataset, we can start looking at some model comparisons. We trained models using both our best hyperparameters and the default hyperparameters, each for 100 epochs. The number of epochs is another important parameter, and the default in Gensim’s implementation is five. So we trained another default word2vec model with only five epochs. Finally, we’ve shown the “Association Rules” baseline, a simple heuristic that predicts recommendations based on frequent co-occurrence between items.</p>
<figure><img src="figures/model_comparison.png" alt="Figure 16: Model comparisons"><figcaption>Figure 16: Model comparisons</figcaption></figure>
<p>It should be no surprise that the best model is the one configured with the hyperparameters discovered during the tuning sweep—but it turns out the number of training epochs is just as crucial.</p>
<p>We can see the relative contribution of this parameter in Figure 16. The blue bar is the Recall@10 score for a word2vec model trained with all default values, including Gensim’s default of only five epochs of training. The orange bar is the same model trained for 100 epochs. This indicates that simply training word2vec for more epochs can give you a big boost in performance, without doing any hyperparameter sweep at all. But if we train for too many epochs, shouldn’t we be worried about overfitting? Not with word2vec.</p>
<p>With traditional neural networks, we set the number of epochs to minimize the training loss (and increase learning) without overfitting (typically indicated by an increase in the validation loss). However, word2vec is different. We do not care necessarily about the training loss because the goal is to learn embeddings for a <em>downstream</em> task. In this sense, it might not actually matter if the model overfits the training data, so long as the resulting embeddings increase performance on the downstream task, according to whatever metrics we’ve implemented. Therefore, it is almost always beneficial to train word2vec <em>for as many epochs as resources allow</em>, or until the downstream task has reached a performance plateau—in which case, additional training does not yield an increase in the downstream metric.</p>
<p>We can see this effect in the figure below, where we plot word2vec’s training loss in grey and the Recall@10 score on the validation set in blue (since there is no validation loss in this case) as a function of training epochs. The Recall@10 score is completely decoupled from word2vec’s training loss and steadily increases over the course of training. By the end of the 100 epochs, this score has relatively flattened. We tried training for longer (200 epochs), but this didn’t significantly increase performance.</p>
<figure><img src="figures/loss_recall_vs_epochs.png" alt="Figure 17: Recall@10 score on the validation set as a function of epochs, where the dark line indicates the mean over 5 models trained on the same (best) hyperparameters, and the light shading is one standard deviation."><figcaption>Figure 17: Recall@10 score on the validation set as a function of epochs, where the dark line indicates the mean over 5 models trained on the same (best) hyperparameters, and the light shading is one standard deviation.</figcaption></figure>
<p>Of course, it’s still better to determine the best set of hyperparameters for the specific dataset you are working with; however, these results demonstrate that you can get far by simply increasing the number of training epochs. We estimate that the increase in training epochs accounts for up to 58% of the performance gains for our best word2vec model (compared to the model trained on default values for only 5 epochs). This suggests that having ideal hyperparameters accounts for roughly 42% of the performance gains, which has performance implications of another kind: computational. Let’s look at some of the challenges of this method for session-based recommendations, starting with the computational cost.</p>
<h3 id="challenges">Challenges</h3>
<p>While using word2vec to learn product embeddings for recommendations is relatively new and somewhat intuitive, it does come with some challenges. Some of these challenges we faced directly in our experiment, but others are likely to crop up when using this approach for different types of datasets or use cases.</p>
<h4 id="computational-resources">Computational Resources</h4>
<p>Using word2vec for product embeddings in a recommendation system can be a viable approach, but for some datasets, identifying the right hyperparameters to optimize those embeddings is a must. The problem is that hyperparameter searches are expensive, requiring one to train hundreds, or even thousands, of model configurations. While the Gensim library<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> is one of the fastest for training vector embeddings, it only supports CPU. GPU support <em>is</em> possible with a Keras wrapper, but benchmarks<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup> indicate it is actually slower than the original Gensim CPU version.<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup></p>
<p>However, there are some strategies to mitigate this challenge:</p>
<blockquote>
<ol>
<li>Perform hyperparameter optimization on a subsample of the original dataset. Although we did not perform this experiment, research<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> has indicated that, for certain datasets, performing hyperparameter optimization on a 10% subsample is sufficient to find good hyperparameters at a fraction of the computational cost.</li>
<li>Perform a smarter hyperparameter sweep using one of many hyperparameter optimization search algorithms. The idea here is that, overall, fewer trials are needed to find the optimal hyperparameters, thus saving CPU hours.</li>
</ol>
</blockquote>
<h4 id="metrics">Metrics</h4>
<p>Recall@<em>K</em> and MRR@<em>K</em> are two common metrics for NEP, and provide a way to assess various models on equal footing. However, they don’t directly measure what a business might truly wish to optimize: revenue, watch/listen time, etc.</p>
<p><strong>Recall@<em>K</em></strong>
This metric is most practical for settings in which the absolute order does not matter (e.g., the recommendations are not highlighted and they are all shown on the screen at the same time). This might be the case for a website that displays ten or twenty recommendations at once, allowing the user to explore options. However, it’s a harsh metric because it doesn’t assign a score to other items that might be nearly identical to the ground truth item. Recall@<em>K</em> simply assigns a 1 or a 0 depending on whether the user’s true choice was included in the list of recommendations. It does not score any number of similar products in that recommendation list that, in reality, might have been equally acceptable to the user.</p>
<p><strong>MRR@<em>K</em></strong>
Mean reciprocal rank is a better choice for applications in which the order of the recommendation matters (e.g., lower ranked recommendations are only shown once the user scrolls to the bottom of the page). In this case, having a higher MRR is crucial, indicating the best recommendations are near the top. Again, this metric only assigns a score if the user’s true choice was included in the list of recommendations and does not give “partial credit” to similar items.</p>
<p>While both of these metrics share similarities with real-world use cases, neither of them directly correlates with increased revenue, watch/listen time, or other real-world KPIs. Furthermore, these metrics do not take into account the <em>quality</em> of the resulting recommendations (more on this in <a href="#online-evaluation">Online evaluation</a>, below). To assess these real-world outcomes, any session-based recommender must be subjected to live, online A/B testing.</p>
<h4 id="the-cold-start-problem">The cold start problem</h4>
<p>The cold start problem afflicts nearly all recommendation systems, and usually comes in two flavors: new users and new products. For session-based recommendation systems, new users are typically not a problem because these systems do not rely explicitly on user characteristics. As long as a website has <strong>some</strong> historical user base, that data can be used to generate embeddings for existing products based on how users navigate the site. These embeddings can then generate reasonable recommendations for new users.</p>
<p>However, new products still present a challenge. These items do not have any sessions associated with them in the training dataset; hence, there are no embeddings associated with them, either. A possible solution to this could be to look at a few similar items (for instance, similar products based on domain, or similar music style) and then assign an embedding that is the average of the embedding vectors of the similar items to create an initial vector.</p>
<h4 id="embeddings-and-scalability">Embeddings and scalability</h4>
<p>A general challenge when using word2vec embeddings is that they can be computationally demanding,<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup> especially compared to simpler heuristics (such as co-occurrence-based recommendations). Embedding methods require substantial amounts of training data to be effective. Also, in order to actually recommend an item, one needs to compute the closest items (based on cosine similarity, nearest neighbor, or some other distance metric) to a given item using the embeddings. This could be challenging to compute in real time. One way to get around it would be to pre-compute and store the <em>K</em>-closest items to an item for easy lookup when needed. There are also indications that embedding methods<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup> may not perform as well as purely sequential models, such as RNNs, although this is likely use-case dependent.</p>
<h2 id="overall-considerations">Overall Considerations</h2>
<p>The definition of a session is crucial to building a successful session-based recommendation system, but this is not always an easy task. Sessions should be determined based on the structure and type of data collected, as well as the use case or problem at hand. Will the sessions be determined from online browsing history, transactions, rated items, or all of the above? How long should a session be? What determines the session boundary? (For instance, if a user is browsing items on the web, sessions could be delineated based on user inactivity.) All of these questions introduce a number of challenges in designing session-based recommenders. This section summarizes the challenges associated with session-based recommendation systems across various session definitions (length, ordering, anonymous vs. non-anonymous, structure, user actions captured, etc.), as well as the challenges in both determining good recommendations and evaluating them.</p>
<h3 id="session-related-issues">Session-related issues</h3>
<p>What is more important: a user’s long-term intent or a short-term intent? To this end, we need to consider session lengths while defining the problem. Session lengths can be roughly categorized into three types: long (&gt; 10 interactions), medium (4-9 interactions) and short (&lt;4 interactions). While more interactions could provide more contextual information, they may also introduce noise because they are likely to contain random interactions (due to uncertain user behavior) which are irrelevant to other interactions, leading to  poor performance. Similarly, shorter sessions yield only a limited amount of information and, consequently, less context. Based on empirical evidence,<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup> medium sessions generally are better at capturing enough contextual information without including too much irrelevant information. This is especially true for transactional data in the ecommerce industry. Selecting an appropriate time span for sessions is domain-dependent, and comes with certain assumptions.</p>
<p>Sessions can also be ordered or unordered, meaning that the user’s interactions may or may not actually capture when the interaction happens. For instance, when shopping online, the order in which a user adds items to their cart may not carry any meaning. Conversely, the sequence of songs listened to may actually matter, reflecting the mood and interests of the user in real time. Such situations warrant looking into appropriate modeling approaches; in some cases co-occurrence-based approaches might work better than sequence models, depending on whether the entries are ordered or unordered.</p>
<p>Another big challenge for session-based recommenders is how to effectively learn from different actions by the user (for instance, how to differentiate between a user’s browsing activity and their purchase activity). Should the session contain only the items that were clicked on? Or should the various actions be combined? If so, how does one model such complex dependencies?</p>
<h3 id="what-determines-a-good-recommendation%3F">What determines a good recommendation?</h3>
<p>Different recommendation strategies and approaches lead to different recommendations. Defining “good” recommendations is challenging because this depends so highly on the use case. For instance, simple baselines use pure co-occurrence between items to generate recommendations, but this might not help a user discover new or lesser known items that might be of interest to them. Additionally, an emphasis on popularity can lead to recommendations that are not diverse.</p>
<p>This is highly relevant in regard to music recommendations. At times, a user may be in the mood to listen to the “most popular” songs, but at other times they are looking to discover something new, in which case popularity-based recommendations are likely not helpful.</p>
<p>Session-based recommenders often tend to make popularity-based recommendations, and this is particularly true of word2vec, which uses co-occurrence within a context to learn item embeddings. These tendencies can be mitigated, in part, by applying popularity or lack-of-diversity penalties to augment the recommendations towards item discovery.<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup>  Another option could be to include a “popularity metric” for evaluating the recommendations. One could measure an algorithm’s popularity by first normalizing popularity values of each item in the training set, and then, during evaluation, computing the popularity by determining the average popularity value of each item that appears in its top-<em>n</em> recommendation list. (Higher values correspondingly mean that an algorithm has a tendency to recommend rather popular items.)<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup></p>
<h3 id="online-evaluation">Online evaluation</h3>
<p>Offline evaluations rarely inform us about the quality of recommendations <strong>as perceived by the users</strong>. In one study of e-commerce session-based recommenders, it was observed that offline evaluation metrics often fall short because they tend to reward an algorithm when they predict the exact item that the user clicked or purchased. In real life, though, there are identical products that could potentially make equally good recommendations. To overcome these limitations, the authors suggest incorporating human feedback on the recommendations from offline evaluations before conducting A/B tests.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Recommender systems have become a cornerstone of online life; they help us navigate an overwhelming amount of information. Being able to predict a user’s interests based on an online session is a highly relevant problem in practice. There is not, however, a one-size-fits-all model; each solution will ultimately be unique to the use case, and care must, as always, be taken in the development and assessment of the model, in order to provide recommendations that will help users and businesses alike. Careful development and assessment of any machine learning model is critical, but it is especially true with recommender systems in general, and session-based recommenders in particular. The best approach is to first benchmark several options on offline metrics (such as Recall@K), and then perform online evaluation (through A/B testing, for instance), before settling on a given model—as offline metrics typically cannot be used to properly assess real-world KPIs (like revenue generation or watch time).</p>
<p>For this report, we experimented with an NLP-based algorithm—word2vec—which is known for learning low-dimensional word representations that are contextual in nature. We applied it to an e-commerce dataset containing historical purchase transactions, to learn the structure induced by both the user’s behavior and the product’s nature to recommend the next item to be purchased. While doing so, we learned that hyperparameter choices are data- and task-dependent, and especially, that they differ from linguistic tasks; what works for language models does not necessarily work for recommendation tasks.</p>
<p>That said, our experiments indicate that in addition to specific parameters (like negative sampling exponent, the number of negative samples, and context window size), the number of training epochs greatly influences model performance. We recommend that word2vec be trained for as many epochs as computational resources allow, or until performance on a downstream recommendation metric has plateaued.</p>
<p>We also realized during our experimentation that performing hyperparameter search over just a handful of parameters can be time consuming and computationally expensive; hence, it could be a bottleneck to developing a real-life recommendation solution (with word2vec). While scaling hyperparameter optimization <em>is</em> possible through tools like Ray Tune, we envision additional research into algorithmic approaches to solve this problem would pave the way in developing more scalable (and less complex) solutions.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/pdf/1704.00023.pdf">On the Reliable Detection of Concept Drift from Streaming Unlabeled Data</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://arxiv.org/pdf/2004.05785.pdf">Learning under Concept Drift: A Review</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Adopted from <a href="https://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/">Applying word2vec to Recommenders and Advertising</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484">Using Word2vec for Music Recommendations</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e">Listing Embeddings in Search Ranking</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="https://arxiv.org/pdf/1606.07154.pdf">E-commerce in Your Inbox:
Product Recommendations at Scale</a> (PDF) <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://arxiv.org/abs/1803.09587">Evaluation of Session-based Recommendation Algorithms</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p><a href="https://arxiv.org/abs/1811.00855">Session-based Recommendation with Graph Neural Networks</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://arxiv.org/abs/1511.06939">Session-based Recommendations with Recurrent Neural Networks</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Example taken from Stanford’s popular CS224 NLP lecture series <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p><a href="https://arxiv.org/abs/1804.04212">Word2Vec applied to Recommendation: Hyperparameters Matter</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p><a href="https://arxiv.org/abs/2009.12192">Tuning Word2vec for Large Scale Recommendation Systems</a> <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="https://arxiv.org/abs/1803.09587">Evaluation of Session-based Recommendation Algorithms</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p><a href="https://www.kaggle.com/vijayuv/onlineretail">https://www.kaggle.com/vijayuv/onlineretail</a> <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p><a href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality</a> <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p><a href="https://radimrehurek.com/gensim/index.html">https://radimrehurek.com/gensim/index.html</a> <a href="#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p><a href="https://rare-technologies.com/gensim-word2vec-on-cpu-faster-than-word2veckeras-on-gpu-incubator-student-blog/">Gensim word2vec on CPU faster than Word2veckeras on GPU</a> <a href="#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>One promising lead is the implementation by AWS, <a href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/">BlazingText</a>, which supports multiple CPUs or GPUs. Their benchmarks claim to dramatically decrease training time. <a href="#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p><a href="https://arxiv.org/abs/2009.12192">Tuning Word2vec for Large Scale Recommendation Systems</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p><a href="https://arxiv.org/abs/1606.08495">Network–Efficient Distributed Word2vec Training System for Large Vocabularies</a> <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p><a href="https://arxiv.org/abs/1803.09587">Evaluation of Session-based Recommendation Algorithms</a> <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p><a href="https://arxiv.org/abs/1902.04864">A Survey on Session-based Recommender Systems</a> <a href="#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p><a href="https://dl.acm.org/doi/10.1145/3097983.3098173">Post Processing Recommender Systems for Diversity</a> <a href="#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p><a href="https://arxiv.org/abs/1910.12781">Empirical Analysis of Session-Based Recommendation Algorithms</a> <a href="#fnref24" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

        </div>
      </body>
   </html>
  