## Evaluating Session-based Recommenders

Evaluation of session-based recommendation systems typically comes in two stages: offline and online evaluation. In offline evaluation, the historical user sessions are typically considered as the “gold standard” for the evaluation. The effectiveness of an algorithm is measured by its ability to predict items withheld from the session. There are a variety of withholding strategies:^[[Evaluation of Session-based Recommendation Algorithms](https://arxiv.org/abs/1803.09587)]
>1. withholding the last element of each session, 
>2. iteratively revealing each interaction in a session, and
>3. in cases where each user has multiple sessions, withholding the entire final session.  

For the purposes of this research report, we have employed withholding the last element of the session. That said, while it is a conceptually simple approach, it may not reflect the user journey throughout a session in the best way. Also, using word2vec for the NEP recommendation task means we are evaluating the embeddings generated by the word2vec model to measure the performance. So, if an item hasn’t been part of the training set, one may have to come up with alternative ways of generating the embedding. (Refer to [Cold Start in Experiments](#cold-start-in-experiments) for more details.)

### Evaluation metrics
When looking at time-ordered sequences of user interactions with the items, we split each sequence into train, validation, and test sets. For a sequence containing n interactions, we use the first (*n*-1) items in that sequence as part of the model training set.  We randomly sample (*n*-1th, *n*th) pairs from these sequences for the validation and test sets. For prediction, we use the last item in the training sequence (the *n*-1th item) as the query item, and predict the *K* closest items to the query item using cosine similarity between the vector representations. We can then evaluate with the following metrics: 

* **Recall at *K* (Recall@*K*)** defined as the proportion of cases in which the ground truth item is among the top *K* recommendations for all test cases (that is, a test example is assigned a score of 1 if the *n*th item appears in the list, and 0 otherwise).

* **Mean Reciprocal Rank (MRR@*K*)**, takes the average of the reciprocal ranks of the ground truth item in among the top *K* recommendations for all test cases (that is, if the *n*th item was second in the list it's reciprocal rank would be 1/2). This metric measures and favors higher ranks in the ordered list of recommendation results.

Accuracy, however, is not the only relevant factor when it comes to recommendations. Depending on the problem, we may want to measure how diverse the recommendations are, or if our algorithm generally tends to recommend most popular items. These additional quality metrics, known as coverage (or diversity) and popularity bias, could help us better understand the potential side-effects of the recommender model.
